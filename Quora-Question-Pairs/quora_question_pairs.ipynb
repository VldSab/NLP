{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quora_question_pairs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cfovb8boCVfo"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.tokens import Doc\n",
        "from spacy.vocab import Vocab\n",
        "import numpy as np\n",
        "from scipy import spatial\n",
        "import pandas as pd\n",
        "import array\n",
        "from sklearn import metrics\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gensim.downloader as api\n",
        "import torch.onnx\n",
        "import onnx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lu7a7HHjDp6O",
        "outputId": "cf97730e-720c-4e60-af29-53da8456e04c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.11.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 11.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.2->onnx) (1.15.0)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUeqY8i9E33a",
        "outputId": "f0111091-162f-4361-a9fb-52bca4b75897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m⚠ Skipping model package dependencies and setting `--no-deps`. You\n",
            "don't seem to have the spaCy package itself installed (maybe because you've\n",
            "built from source?), so installing the model dependencies would cause spaCy to\n",
            "be downloaded, which probably isn't what you want. If the model package has\n",
            "other dependencies, you'll have to install them manually.\u001b[0m\n",
            "Collecting en_core_web_md==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4 MB 1.7 MB/s \n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "\u001b[?25h^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_md\")"
      ],
      "metadata": {
        "id": "maJcdlwuCc88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(nlp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQOwUkVsId30",
        "outputId": "5a4d255e-019c-4e53-9931-6513241500c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.lang.en.English"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"train.csv\", engine='python', error_bad_lines=False, index_col='id')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTkbPKGl2-ko",
        "outputId": "aa339161-b243-414e-8aa5-dadeac83f016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "54otxXJE3Dpe",
        "outputId": "d358c50a-cd3b-4542-c19a-3788a8be1490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    qid1  qid2                                          question1  \\\n",
              "id                                                                  \n",
              "0      1     2  What is the step by step guide to invest in sh...   \n",
              "1      3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
              "2      5     6  How can I increase the speed of my internet co...   \n",
              "3      7     8  Why am I mentally very lonely? How can I solve...   \n",
              "4      9    10  Which one dissolve in water quikly sugar, salt...   \n",
              "\n",
              "                                            question2  is_duplicate  \n",
              "id                                                                   \n",
              "0   What is the step by step guide to invest in sh...             0  \n",
              "1   What would happen if the Indian government sto...             0  \n",
              "2   How can Internet speed be increased by hacking...             0  \n",
              "3   Find the remainder when [math]23^{24}[/math] i...             0  \n",
              "4             Which fish would survive in salt water?             0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4279f975-1361-45b4-81ac-c9a811d56e15\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4279f975-1361-45b4-81ac-c9a811d56e15')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4279f975-1361-45b4-81ac-c9a811d56e15 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4279f975-1361-45b4-81ac-c9a811d56e15');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = df[['question1', 'question2']]"
      ],
      "metadata": {
        "id": "9zqbPVRj3HEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_list = list(zip(list(data['question1']), list(data['question2'])))"
      ],
      "metadata": {
        "id": "pv6z2GH63KKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_list[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZQr_2H0TuJY",
        "outputId": "b861cef1-e297-41ea-ad78-cc827080d747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('What is the step by step guide to invest in share market in india?',\n",
              " 'What is the step by step guide to invest in share market?')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp(data_list[0][0]).tensor[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66H2FGYvTmiR",
        "outputId": "bae0cc30-f617-4472-fae8-3afafd02d8d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.85502553e-01, -1.04933894e+00, -1.83714175e+00,\n",
              "        -1.38754904e-01,  2.21111536e-01,  8.21512699e-01,\n",
              "         3.84804320e+00,  1.57037091e+00,  1.40238762e+00,\n",
              "        -1.51353669e+00, -1.85361576e+00, -1.15423346e+00,\n",
              "        -2.25412345e+00,  4.45111418e+00,  2.78489137e+00,\n",
              "         6.87332153e-02, -1.41078544e+00, -4.17306376e+00,\n",
              "        -2.23620820e+00,  4.89340115e+00,  1.78866506e-01,\n",
              "        -4.47662979e-01,  2.89564228e+00, -2.07077599e+00,\n",
              "         3.76245403e+00,  4.56226349e-01, -2.53331780e+00,\n",
              "         4.49030459e-01, -1.77216172e+00,  1.70961380e+00,\n",
              "        -6.01698351e+00,  1.17936909e+00, -1.40537155e+00,\n",
              "        -2.93449712e+00,  1.24626708e+00, -1.67899704e+00,\n",
              "         2.57465506e+00,  7.26307154e-01, -1.65509117e+00,\n",
              "        -1.17399085e+00,  2.05376506e-01, -2.72255468e+00,\n",
              "         1.80539656e+00, -2.25418001e-01, -3.74671793e+00,\n",
              "        -6.32416844e-01,  7.33165979e-01, -1.11582708e+00,\n",
              "         1.29762232e+00, -7.99497366e-02, -4.15477753e+00,\n",
              "        -1.90298438e-01, -9.69062805e-01,  3.45621085e+00,\n",
              "         1.55714369e+00,  1.16281104e+00, -2.30399275e+00,\n",
              "         9.72364545e-01, -2.06444621e+00, -8.57262552e-01,\n",
              "         7.33939648e-01, -7.96230972e-01,  2.89970517e+00,\n",
              "         2.49257731e+00,  1.68445373e+00, -1.06762171e+00,\n",
              "        -8.41384172e-01,  4.08179235e+00, -7.73943901e-01,\n",
              "        -1.78078544e+00, -2.54898131e-01,  1.35772097e+00,\n",
              "         3.83452892e+00, -1.49584675e+00,  4.38856781e-01,\n",
              "        -4.09343815e+00,  3.08462238e+00, -2.68401957e+00,\n",
              "         2.63605499e+00, -1.45050478e+00, -8.73637676e-01,\n",
              "         3.20799398e+00,  2.58636332e+00,  1.30534899e+00,\n",
              "        -1.06956863e+00, -3.62134933e+00,  7.11709976e+00,\n",
              "        -1.08599722e+00,  5.44326186e-01, -8.54952216e-01,\n",
              "        -2.41192722e+00,  3.12543809e-01,  1.83590472e+00,\n",
              "        -9.74495053e-01, -8.83501232e-01,  3.13978672e+00],\n",
              "       [ 1.45912170e+00,  2.50262260e+00,  2.78204143e-01,\n",
              "        -2.08854723e+00,  3.61751497e-01,  4.03010464e+00,\n",
              "        -4.80397320e+00,  3.68297100e-02, -2.84697652e+00,\n",
              "        -1.91419697e+00,  2.27492046e+00, -3.78746748e+00,\n",
              "        -1.40499997e+00,  7.14495778e-02,  1.91595459e+00,\n",
              "         3.81936371e-01,  3.39520097e+00,  2.11465776e-01,\n",
              "         6.61749649e+00, -4.11812115e+00, -1.12361491e-01,\n",
              "        -2.30673218e+00, -2.58177966e-01, -1.37718821e+00,\n",
              "        -4.68702602e+00,  1.39201391e+00, -5.20306635e+00,\n",
              "        -5.26597500e+00,  3.75815225e+00,  2.49960423e+00,\n",
              "        -1.43790674e+00, -2.18977594e+00,  8.83372128e-01,\n",
              "         1.69134605e+00, -6.20490015e-01,  7.97532916e-01,\n",
              "        -1.58754706e+00, -1.61620712e+00, -2.09938645e+00,\n",
              "         1.22451335e-01, -3.45453548e+00,  5.53654337e+00,\n",
              "        -9.50333834e-01,  4.73447037e+00,  6.69234133e+00,\n",
              "         2.65664721e+00, -1.15390134e+00, -2.66406608e+00,\n",
              "         8.90241802e-01, -1.43304062e+00, -1.16790783e+00,\n",
              "        -4.72622299e+00,  4.29949808e+00, -1.03853059e+00,\n",
              "         9.54736233e-01,  1.78044724e+00, -3.36097717e+00,\n",
              "         1.13507867e+00,  3.09336185e-03, -1.70301700e+00,\n",
              "        -1.98252499e+00, -1.94075680e+00,  1.21857584e+00,\n",
              "        -1.66920364e-01,  4.23451805e+00,  3.30516338e-01,\n",
              "         1.84778762e+00, -4.20948863e-01,  1.93431795e+00,\n",
              "         4.61511326e+00, -1.14885736e+00,  1.68105805e+00,\n",
              "        -1.69726348e+00,  8.31990778e-01,  2.48802304e+00,\n",
              "         2.91120678e-01,  9.01246667e-01,  3.66925836e-01,\n",
              "         1.82417917e+00,  2.35410124e-01,  1.34144521e+00,\n",
              "         3.39434624e-01,  5.15168858e+00, -2.97416210e+00,\n",
              "        -1.50998628e+00, -2.98946083e-01, -1.36753953e+00,\n",
              "         2.34809786e-01, -1.81587172e+00,  4.58535004e+00,\n",
              "        -7.84221828e-01, -2.24056435e+00, -1.85134780e+00,\n",
              "         3.11661434e+00, -3.06448054e+00, -5.69294572e-01],\n",
              "       [ 1.75864458e+00, -3.16612864e+00, -3.96039701e+00,\n",
              "        -2.66386557e+00,  2.44919443e+00,  2.09294653e+00,\n",
              "         1.87694180e+00, -2.27496386e+00,  3.21330929e+00,\n",
              "        -2.08268023e+00, -1.72181201e+00,  2.30329394e+00,\n",
              "         4.95020771e+00,  1.56205106e+00, -6.45979196e-02,\n",
              "        -2.12789822e+00, -1.54492426e+00,  9.83155966e-01,\n",
              "         1.16919518e+00, -1.63237500e+00, -1.32558548e+00,\n",
              "        -2.78867865e+00,  6.56761026e+00, -2.51947927e+00,\n",
              "        -2.01820523e-01,  4.30738068e+00, -1.33927822e-01,\n",
              "         1.26429367e+00, -2.56965804e+00,  5.53569794e-02,\n",
              "         3.31022310e+00, -2.92014313e+00, -1.02937984e+00,\n",
              "        -1.11641335e+00,  8.33078086e-01, -4.67305005e-01,\n",
              "        -8.05684388e-01,  4.30107474e-01, -9.97945666e-01,\n",
              "        -1.22214031e+00, -1.61919844e+00, -2.38543439e+00,\n",
              "        -1.03147030e-01, -1.15142047e+00, -6.17230177e+00,\n",
              "         3.69527555e+00,  4.27319384e+00, -4.34684098e-01,\n",
              "         8.71438086e-01, -2.59935975e-01, -1.00513530e+00,\n",
              "         6.54523909e-01, -2.43226051e+00,  3.94932449e-01,\n",
              "        -1.89275324e-01,  2.55064130e+00, -2.75031304e+00,\n",
              "         4.48355675e+00,  7.88598299e-01,  7.45344543e+00,\n",
              "        -3.00742674e+00, -1.01769340e+00, -2.96991944e+00,\n",
              "        -3.61878991e+00,  2.51485586e-01,  1.54115391e+00,\n",
              "         4.03466702e+00,  6.47808194e-01,  1.83959496e+00,\n",
              "        -3.09691882e+00, -3.06363726e+00, -1.02406526e+00,\n",
              "        -2.12970161e+00,  3.33607864e+00, -1.61949623e+00,\n",
              "        -8.31911385e-01,  2.75569057e+00, -7.25276649e-01,\n",
              "         8.75483215e-01,  7.39709258e-01, -2.07610250e-01,\n",
              "         3.09375572e+00, -8.78987908e-01, -3.16373634e+00,\n",
              "        -2.95521975e-01, -2.74438381e-01,  6.93862319e-01,\n",
              "        -1.83768660e-01, -3.02756524e+00, -1.82966220e+00,\n",
              "         3.73936534e+00,  4.26072240e-01, -1.35957837e+00,\n",
              "         2.13201523e+00,  2.29990458e+00,  3.70160103e-01],\n",
              "       [ 2.83994389e+00,  2.00193667e+00,  1.14009953e+00,\n",
              "         2.55333424e+00,  9.83311296e-01, -1.05115354e+00,\n",
              "         1.53615081e+00, -2.13395095e+00, -3.83604717e+00,\n",
              "        -1.39269054e-01,  1.46603251e+00,  9.02589321e-01,\n",
              "         4.65964460e+00, -1.74987316e-03, -1.84140831e-01,\n",
              "         1.95538497e+00,  7.09001064e-01, -1.52579069e+00,\n",
              "        -2.81775117e-01,  9.95256960e-01, -3.81369305e+00,\n",
              "         1.46988738e+00, -9.59214509e-01, -1.45394146e+00,\n",
              "         1.58042490e-01,  5.56754231e-01,  3.20505738e+00,\n",
              "        -1.29239357e+00, -1.82943976e+00,  1.55493808e+00,\n",
              "        -7.31106699e-02,  1.15597105e+00,  2.83328652e+00,\n",
              "        -2.62108898e+00,  1.57648420e+00,  3.01797438e+00,\n",
              "         1.36757481e+00,  2.18640423e+00, -5.06689310e-01,\n",
              "         1.93522584e+00, -1.84741020e-01,  6.00099564e-01,\n",
              "         3.37728262e+00, -1.58166552e+00, -2.17714477e+00,\n",
              "         1.10461593e+00,  4.32651639e-01, -3.78838682e+00,\n",
              "        -2.29361996e-01,  2.00869966e+00,  1.54967594e+00,\n",
              "         1.24772072e+00, -2.38552523e+00, -3.49381256e+00,\n",
              "        -2.34652805e+00, -9.43624377e-02,  3.55343175e+00,\n",
              "         1.32376599e+00, -1.38267183e+00,  1.52596962e+00,\n",
              "        -1.83975244e+00, -2.24398851e+00, -2.66162539e+00,\n",
              "        -4.62949467e+00, -1.76060390e+00, -2.39933419e+00,\n",
              "        -9.84035134e-01,  3.16375196e-01, -2.05584478e+00,\n",
              "         4.78598207e-01, -2.83379364e+00,  2.95972013e+00,\n",
              "        -2.19747996e+00,  2.47708130e+00, -4.06912565e+00,\n",
              "         1.99779284e+00,  2.17655253e+00,  7.66423941e-01,\n",
              "        -5.80312347e+00,  1.03685057e+00, -1.30353856e+00,\n",
              "        -3.47709864e-01, -1.68329310e+00, -2.11703563e+00,\n",
              "         2.21632886e+00, -1.67050552e+00,  3.43467665e+00,\n",
              "         1.52215266e+00,  4.11873913e+00,  6.48390055e-02,\n",
              "        -2.02166963e+00,  2.28081179e+00,  4.54099989e+00,\n",
              "         3.00934887e+00, -2.96807337e+00, -4.08414078e+00],\n",
              "       [ 2.75724053e+00,  8.62656713e-01, -5.75855970e-01,\n",
              "         2.67643046e+00,  1.85864699e+00,  3.92600560e+00,\n",
              "        -1.36272347e+00, -3.62532878e+00, -4.19131947e+00,\n",
              "        -1.68079472e+00, -3.05039287e-01, -1.49297500e+00,\n",
              "        -6.50854528e-01,  3.96491528e+00, -3.36198926e-01,\n",
              "         4.41219711e+00,  3.09755659e+00, -6.26644492e-01,\n",
              "         4.04310703e+00, -1.52944207e+00, -9.03277993e-01,\n",
              "         2.50203133e+00, -4.72571611e-01, -2.46748805e+00,\n",
              "        -2.72800279e+00, -3.24337101e+00,  2.45742515e-01,\n",
              "        -1.79936445e+00,  1.54699671e+00, -2.80478507e-01,\n",
              "         3.18560457e+00, -2.37014627e+00, -1.52061558e+00,\n",
              "        -1.22282672e+00,  5.05107701e-01,  1.65642607e+00,\n",
              "        -4.63838959e+00, -1.47151446e+00, -3.31779504e+00,\n",
              "        -1.48635709e+00, -2.20198321e+00,  5.21584368e+00,\n",
              "        -5.17232716e-02, -3.39233470e+00, -4.01993513e+00,\n",
              "         6.13635182e-01,  7.96040058e-01,  1.24924779e+00,\n",
              "         2.66163230e+00,  4.74373055e+00, -1.52082384e-01,\n",
              "         1.46179891e+00,  1.37148201e-01, -1.29689157e+00,\n",
              "        -1.53313613e+00,  9.07450914e-02, -1.85036922e+00,\n",
              "         3.50310516e+00,  1.08474123e+00,  3.06356788e+00,\n",
              "        -2.57556391e+00,  1.54873109e+00, -1.09867060e+00,\n",
              "         9.45372224e-01, -1.74689353e+00,  2.37980753e-01,\n",
              "         3.06721139e+00, -3.45461655e+00, -3.24256968e+00,\n",
              "        -2.99117160e+00, -9.38472033e-01, -1.13190651e+00,\n",
              "         1.69570148e-01,  1.48374581e+00, -2.06910872e+00,\n",
              "        -1.23680687e+00,  5.93036234e-01,  3.10554504e+00,\n",
              "         1.51817405e+00,  1.13020813e+00, -2.28112316e+00,\n",
              "         3.75768757e+00, -9.54129577e-01,  1.49098396e+00,\n",
              "        -2.48336911e+00,  2.17121816e+00, -6.81631684e-01,\n",
              "        -4.67395306e+00, -2.63381958e+00,  8.40124416e+00,\n",
              "         3.46005869e+00, -2.54990876e-01, -1.55021000e+00,\n",
              "         1.96797395e+00,  4.17776108e+00, -2.99918795e+00]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First approach \n",
        "Make embeddings for each word in sentence, then calculate mean value for sentence. Finally calculate cosine distance between two vectors."
      ],
      "metadata": {
        "id": "ObxC-zGx2upE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# delete stop words, lowercase and tokenize\n",
        "def tokenize(sentence: str, nlp):\n",
        "  stop_words = nlp.Defaults.stop_words\n",
        "  tokens = sentence.split()\n",
        "  sentence_without_sw = ' '.join([token.lower() for token in tokens if token not in stop_words])\n",
        "  tokens_without_sw = nlp(sentence_without_sw)\n",
        "  return tokens_without_sw"
      ],
      "metadata": {
        "id": "OomwqsAQgw7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'My name is definetly Vova'\n",
        "tokenize(sentence, nlp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQpf2GUmijzX",
        "outputId": "1697be94-fefd-485d-da65-df67a771ff54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "my definetly vova"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(sentence1: str, sentence2: str, nlp) -> int:\n",
        "  embedding1 = tokenize(sentence1, nlp).tensor.mean(axis=0)\n",
        "  embedding2 = tokenize(sentence2, nlp).tensor.mean(axis=0)\n",
        "  return spatial.distance.cosine(embedding1, embedding2)"
      ],
      "metadata": {
        "id": "9FhgvdlnHoIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(data_list, nlp):\n",
        "  result = []\n",
        "  for tup in data_list[:10000]:\n",
        "    result.append(cosine_similarity(tup[0], tup[1], nlp))\n",
        "  result = np.array(result)\n",
        "  result /= max(result)\n",
        "\n",
        "  return (result > 0.5).astype(int)"
      ],
      "metadata": {
        "id": "c99mXsd8OAgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_list = predict(data_list, nlp)"
      ],
      "metadata": {
        "id": "9uv38vk9v2Z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.array(df['is_duplicate'][:10000])"
      ],
      "metadata": {
        "id": "2ENTRyyWzedN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.accuracy_score(predicted_list, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0hQIElf0I3u",
        "outputId": "afe8797b-3975-45f3-f345-a220d4f064d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5445"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, it is not good approach for this task. Accuracy 0.54 is too small."
      ],
      "metadata": {
        "id": "ScUlOeKLrr_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now use spacy method"
      ],
      "metadata": {
        "id": "7nUmaDNopWP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'My name is definetly Vova'\n",
        "sentence2 = 'My name is Vova'\n",
        "toks1 = nlp(sentence)\n",
        "toks2 = nlp(sentence2)\n",
        "toks1.similarity(toks2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YcwH9EUpqxT",
        "outputId": "ae166a81-644d-4728-abee-2af450a1bbcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9631748208608146"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_spacy(data_list, nlp):\n",
        "  result = []\n",
        "  for tup in data_list[:10000]:\n",
        "    result.append(nlp(tup[0]).similarity(nlp(tup[1])))\n",
        "  result = np.array(result)\n",
        "  return (result < 0.5).astype(int)"
      ],
      "metadata": {
        "id": "pfCgOF1ipVJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_list_spacy = predict_spacy(data_list, nlp)"
      ],
      "metadata": {
        "id": "wD2vAiJeqdjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.accuracy_score(predicted_list_spacy, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OynLYk7qzgq",
        "outputId": "03f01d75-1f38-4425-8aef-29c2a25d1445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6274"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results with spaCy approach a bit better "
      ],
      "metadata": {
        "id": "usQqVZ0w3Pxt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second approach: LSTM\n",
        "1. Train our own model for encoding\n",
        "2. Use pre-trained encoder"
      ],
      "metadata": {
        "id": "O-TlzjWB3m1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Own model"
      ],
      "metadata": {
        "id": "-vvWoXU9OKrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "NLiYTywE3ciV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCH = 80\n",
        "NUM_LAYERS = 3\n",
        "NUM_CLASSES = 6\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "#INPUT_SIZE = 96\n",
        "INPUT_SIZE = 300\n",
        "HIDDEN_SIZE = 128\n",
        "SEQUENCE_LEN = 25\n",
        "\n",
        "LEARNING_RATE = 0.001"
      ],
      "metadata": {
        "id": "5mElsahFZErc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create embeddings with spaCy\n",
        "def create_embeddings(data, nlp, column_number, number_of_examples):\n",
        "  embeddings_list = []\n",
        "  idx = 0\n",
        "  for sentence in list(data.iloc[:, column_number])[:number_of_examples]:\n",
        "    if not isinstance(sentence, str):\n",
        "      work_sentence = str(sentence)[:SEQUENCE_LEN]\n",
        "    else:\n",
        "      work_sentence = sentence[:SEQUENCE_LEN]\n",
        "    if idx != 0 and idx % 2000 == 0:\n",
        "      print(idx)\n",
        "    #try:\n",
        "    spacy_embedded_sentence = nlp(work_sentence)\n",
        "    #except Exception:\n",
        "    #  print(Exception)\n",
        "    #  continue\n",
        "    tensor_width = spacy_embedded_sentence.tensor.shape[0]\n",
        "    #if tensor_width < SEQUENCE_LEN:\n",
        "    embedded_sentence = torch.zeros(SEQUENCE_LEN, INPUT_SIZE)\n",
        "    embedded_sentence[:tensor_width, :] = torch.from_numpy(spacy_embedded_sentence.tensor)\n",
        "    # else:\n",
        "    #   embedded_sentence = torch.zeros(SEQUENCE_LEN, INPUT_SIZE)\n",
        "    #   embedded_sentence = torch.from_numpy(spacy_embedded_sentence.tensor[:SEQUENCE_LEN, :])\n",
        "    embeddings_list.append(torch.unsqueeze(embedded_sentence, 0))\n",
        "    idx += 1\n",
        "  return torch.cat(embeddings_list, dim=0)"
      ],
      "metadata": {
        "id": "rma40eWSXeA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_w2v = api.load(\"word2vec-google-news-300\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F_nFRQt1sxh",
        "outputId": "9273c3bc-caa3-497e-ff43-8911d922cb3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create embeddings with gensim\n",
        "def create_embeddings_gensim(data, column_number, number_of_examples):\n",
        "  embedded_sentences = []\n",
        "  for i, sentence in enumerate(list(data.iloc[:, column_number])[:number_of_examples]):\n",
        "    if i % 2000 == 0:\n",
        "      print(i)\n",
        "    temp_sentence = []\n",
        "    if not isinstance(sentence, str):\n",
        "      temp_sentence = str(sentence)[:SEQUENCE_LEN]\n",
        "      if len(temp_sentence) < SEQUENCE_LEN:\n",
        "        temp_sentence += ' ' * (SEQUENCE_LEN - len(temp_sentence))\n",
        "    else:\n",
        "      temp_sentence = sentence[:SEQUENCE_LEN]\n",
        "      if len(temp_sentence) < SEQUENCE_LEN:\n",
        "        temp_sentence += ' ' * (SEQUENCE_LEN - len(temp_sentence))\n",
        "    embedded_sentence = []\n",
        "    for word in temp_sentence:\n",
        "      try:\n",
        "        embedding = model_w2v.get_vector(word)\n",
        "      except:\n",
        "        embedding = np.zeros(300)\n",
        "      embedded_sentence.append(torch.unsqueeze(torch.tensor(embedding, dtype=torch.float), 0))\n",
        "    embedded_sentences.append(torch.unsqueeze(torch.cat(embedded_sentence, dim=0), 0))\n",
        "  return torch.cat(embedded_sentences, dim=0)"
      ],
      "metadata": {
        "id": "Y4WfisJwWsvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#here is bidirectional LSTM\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, num_classes) -> None:\n",
        "      super(LSTM, self).__init__()\n",
        "      self.input_size = input_size\n",
        "      self.hidden_size = hidden_size\n",
        "      self.num_layers = num_layers\n",
        "      self.dropout = 0.3\n",
        "      self.LSTM = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True, dropout = self.dropout)\n",
        "      self.fc_1 = nn.Linear(hidden_size * 2, 128)\n",
        "      self.fc_2 = nn.Linear(128, 128)\n",
        "      self.fc = nn.Linear(128, num_classes)\n",
        "      # x needs to be: (batch_size, seq, input_size)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    #init hidden and cell states for lstm\n",
        "    #(num_layers, batch_size, hidden_size)\n",
        "    # multiply x2 because here is BIDIRECTIONAL LSTM\n",
        "    h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device) \n",
        "    c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device) \n",
        "\n",
        "    x = x.to(device)\n",
        "    out, _ = self.LSTM(x, (h0, c0)) \n",
        "    #out -> (batch_size, seq_length, hidden_size)\n",
        "    #Decode the hidden state of the last time step\n",
        "    out = out[:, -1, :]\n",
        "    out = self.fc_1(out)\n",
        "    out = self.fc_2(out)\n",
        "    out = self.fc(out)\n",
        "    # out: (n, num_classes)\n",
        "    return out\n",
        "\n",
        "  def backward(self, out, labels, criterion, optimizer):\n",
        "    optimizer.zero_grad()\n",
        "    labels = labels.to(device)\n",
        "    loss = criterion(out, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "cmV2JH6X_PVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, num_epochs, data_loader, criterion, optimizer):\n",
        "  total_loss = 0\n",
        "  loss_list = []\n",
        "  total_steps = num_epochs * len(data_loader)\n",
        "  for epoch in range(num_epochs):\n",
        "    for i, (x, labels) in enumerate(data_loader):\n",
        "      out = model.forward(x)\n",
        "      loss = model.backward(out, labels, criterion, optimizer)\n",
        "      total_loss += loss.item()\n",
        "      loss_list.append(loss.item())\n",
        "      if (i+1) % 10 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{epoch}/{total_steps}], Loss: {loss.item():.4f}')\n",
        "  return loss_list, total_loss"
      ],
      "metadata": {
        "id": "XHJTl6CrG7X_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare train dataset\n",
        "data_encoder = pd.read_csv('train_encoder.csv', index_col='id')\n",
        "data_encoder.dropna()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "_spS7bjnOdKj",
        "outputId": "b50af745-2b12-4ec2-df45-46f4308eab74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            comment_text  toxicity\n",
              "id                                                                \n",
              "0                     fuck you you self righteous creep          3\n",
              "1       stop stop the goddam vandalism or there ll be...         2\n",
              "2      i agree rt does have a few shortcomings  but i...         0\n",
              "3      if you would like verfiability here is the lin...         0\n",
              "4      do you think there s consensus for me to be on...         0\n",
              "...                                                  ...       ...\n",
              "34642  huy i am kyle robbins i think albert pujols is...         4\n",
              "34643  unlike the ancient greeks  the idiot who  bloc...         4\n",
              "34644  no it isn t  so if you wish to delete my accou...         1\n",
              "34645    you haven t been paying attention  i don t c...         1\n",
              "34646   you are the racist piece of dirt buckley  and...         3\n",
              "\n",
              "[34646 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ab223e0d-1bea-4d88-aed1-99085a0ad47f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxicity</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fuck you you self righteous creep</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>stop stop the goddam vandalism or there ll be...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i agree rt does have a few shortcomings  but i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>if you would like verfiability here is the lin...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>do you think there s consensus for me to be on...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34642</th>\n",
              "      <td>huy i am kyle robbins i think albert pujols is...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34643</th>\n",
              "      <td>unlike the ancient greeks  the idiot who  bloc...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34644</th>\n",
              "      <td>no it isn t  so if you wish to delete my accou...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34645</th>\n",
              "      <td>you haven t been paying attention  i don t c...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34646</th>\n",
              "      <td>you are the racist piece of dirt buckley  and...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>34646 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab223e0d-1bea-4d88-aed1-99085a0ad47f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ab223e0d-1bea-4d88-aed1-99085a0ad47f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ab223e0d-1bea-4d88-aed1-99085a0ad47f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#features = create_embeddings(data_encoder, nlp, 0, 25000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1W35epRPEy7",
        "outputId": "4f09c7c9-0402-4176-e2e3-f1ef90a6d9d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = create_embeddings_gensim(data_encoder, 0, 25000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5H4457AyfBt",
        "outputId": "ace29ad2-2ea7-4119-ed80-58c4bb61f2a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "3500\n",
            "3600\n",
            "3700\n",
            "3800\n",
            "3900\n",
            "4000\n",
            "4100\n",
            "4200\n",
            "4300\n",
            "4400\n",
            "4500\n",
            "4600\n",
            "4700\n",
            "4800\n",
            "4900\n",
            "5000\n",
            "5100\n",
            "5200\n",
            "5300\n",
            "5400\n",
            "5500\n",
            "5600\n",
            "5700\n",
            "5800\n",
            "5900\n",
            "6000\n",
            "6100\n",
            "6200\n",
            "6300\n",
            "6400\n",
            "6500\n",
            "6600\n",
            "6700\n",
            "6800\n",
            "6900\n",
            "7000\n",
            "7100\n",
            "7200\n",
            "7300\n",
            "7400\n",
            "7500\n",
            "7600\n",
            "7700\n",
            "7800\n",
            "7900\n",
            "8000\n",
            "8100\n",
            "8200\n",
            "8300\n",
            "8400\n",
            "8500\n",
            "8600\n",
            "8700\n",
            "8800\n",
            "8900\n",
            "9000\n",
            "9100\n",
            "9200\n",
            "9300\n",
            "9400\n",
            "9500\n",
            "9600\n",
            "9700\n",
            "9800\n",
            "9900\n",
            "10000\n",
            "10100\n",
            "10200\n",
            "10300\n",
            "10400\n",
            "10500\n",
            "10600\n",
            "10700\n",
            "10800\n",
            "10900\n",
            "11000\n",
            "11100\n",
            "11200\n",
            "11300\n",
            "11400\n",
            "11500\n",
            "11600\n",
            "11700\n",
            "11800\n",
            "11900\n",
            "12000\n",
            "12100\n",
            "12200\n",
            "12300\n",
            "12400\n",
            "12500\n",
            "12600\n",
            "12700\n",
            "12800\n",
            "12900\n",
            "13000\n",
            "13100\n",
            "13200\n",
            "13300\n",
            "13400\n",
            "13500\n",
            "13600\n",
            "13700\n",
            "13800\n",
            "13900\n",
            "14000\n",
            "14100\n",
            "14200\n",
            "14300\n",
            "14400\n",
            "14500\n",
            "14600\n",
            "14700\n",
            "14800\n",
            "14900\n",
            "15000\n",
            "15100\n",
            "15200\n",
            "15300\n",
            "15400\n",
            "15500\n",
            "15600\n",
            "15700\n",
            "15800\n",
            "15900\n",
            "16000\n",
            "16100\n",
            "16200\n",
            "16300\n",
            "16400\n",
            "16500\n",
            "16600\n",
            "16700\n",
            "16800\n",
            "16900\n",
            "17000\n",
            "17100\n",
            "17200\n",
            "17300\n",
            "17400\n",
            "17500\n",
            "17600\n",
            "17700\n",
            "17800\n",
            "17900\n",
            "18000\n",
            "18100\n",
            "18200\n",
            "18300\n",
            "18400\n",
            "18500\n",
            "18600\n",
            "18700\n",
            "18800\n",
            "18900\n",
            "19000\n",
            "19100\n",
            "19200\n",
            "19300\n",
            "19400\n",
            "19500\n",
            "19600\n",
            "19700\n",
            "19800\n",
            "19900\n",
            "20000\n",
            "20100\n",
            "20200\n",
            "20300\n",
            "20400\n",
            "20500\n",
            "20600\n",
            "20700\n",
            "20800\n",
            "20900\n",
            "21000\n",
            "21100\n",
            "21200\n",
            "21300\n",
            "21400\n",
            "21500\n",
            "21600\n",
            "21700\n",
            "21800\n",
            "21900\n",
            "22000\n",
            "22100\n",
            "22200\n",
            "22300\n",
            "22400\n",
            "22500\n",
            "22600\n",
            "22700\n",
            "22800\n",
            "22900\n",
            "23000\n",
            "23100\n",
            "23200\n",
            "23300\n",
            "23400\n",
            "23500\n",
            "23600\n",
            "23700\n",
            "23800\n",
            "23900\n",
            "24000\n",
            "24100\n",
            "24200\n",
            "24300\n",
            "24400\n",
            "24500\n",
            "24600\n",
            "24700\n",
            "24800\n",
            "24900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JV2rEot5CErl",
        "outputId": "5cd5d884-cd3d-46a9-ea6a-c2971fda7152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([25, 300])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(model_w2v.get_vector('albert'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__Ep5J5u1a6o",
        "outputId": "c120b467-6262-4204-8a6c-7bd61f2f8835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = torch.tensor(list(data_encoder['toxicity']), dtype=torch.long)\n",
        "train_tensor_dataset = TensorDataset(features, labels[:25000])\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset=train_tensor_dataset, \n",
        "                                           batch_size=BATCH_SIZE, \n",
        "                                           shuffle=True)"
      ],
      "metadata": {
        "id": "HaeiI46wPE1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train \n",
        "model = LSTM(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, NUM_CLASSES).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  "
      ],
      "metadata": {
        "id": "pfwTYoZvUDUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list, _ = train_model(model, NUM_EPOCH, train_dataloader, criterion, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9zjW6JYUDWx",
        "outputId": "189ba978-02e6-4b39-a5ed-900d9837b882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/80], Step [0/7840], Loss: 1.4750\n",
            "Epoch [1/80], Step [0/7840], Loss: 1.4892\n",
            "Epoch [1/80], Step [0/7840], Loss: 1.4505\n",
            "Epoch [1/80], Step [0/7840], Loss: 1.4005\n",
            "Epoch [1/80], Step [0/7840], Loss: 1.4131\n",
            "Epoch [1/80], Step [0/7840], Loss: 1.3178\n",
            "Epoch [1/80], Step [0/7840], Loss: 1.3785\n",
            "Epoch [1/80], Step [0/7840], Loss: 1.3571\n",
            "Epoch [1/80], Step [0/7840], Loss: 1.3542\n",
            "Epoch [2/80], Step [1/7840], Loss: 1.2900\n",
            "Epoch [2/80], Step [1/7840], Loss: 1.3703\n",
            "Epoch [2/80], Step [1/7840], Loss: 1.2739\n",
            "Epoch [2/80], Step [1/7840], Loss: 1.2951\n",
            "Epoch [2/80], Step [1/7840], Loss: 1.3515\n",
            "Epoch [2/80], Step [1/7840], Loss: 1.3207\n",
            "Epoch [2/80], Step [1/7840], Loss: 1.3887\n",
            "Epoch [2/80], Step [1/7840], Loss: 1.2386\n",
            "Epoch [2/80], Step [1/7840], Loss: 1.2702\n",
            "Epoch [3/80], Step [2/7840], Loss: 1.2757\n",
            "Epoch [3/80], Step [2/7840], Loss: 1.3516\n",
            "Epoch [3/80], Step [2/7840], Loss: 1.2166\n",
            "Epoch [3/80], Step [2/7840], Loss: 1.2058\n",
            "Epoch [3/80], Step [2/7840], Loss: 1.2979\n",
            "Epoch [3/80], Step [2/7840], Loss: 1.2546\n",
            "Epoch [3/80], Step [2/7840], Loss: 1.2972\n",
            "Epoch [3/80], Step [2/7840], Loss: 1.2950\n",
            "Epoch [3/80], Step [2/7840], Loss: 1.2710\n",
            "Epoch [4/80], Step [3/7840], Loss: 1.3391\n",
            "Epoch [4/80], Step [3/7840], Loss: 1.3568\n",
            "Epoch [4/80], Step [3/7840], Loss: 1.2913\n",
            "Epoch [4/80], Step [3/7840], Loss: 1.2719\n",
            "Epoch [4/80], Step [3/7840], Loss: 1.2089\n",
            "Epoch [4/80], Step [3/7840], Loss: 1.2399\n",
            "Epoch [4/80], Step [3/7840], Loss: 1.2855\n",
            "Epoch [4/80], Step [3/7840], Loss: 1.2441\n",
            "Epoch [4/80], Step [3/7840], Loss: 1.2635\n",
            "Epoch [5/80], Step [4/7840], Loss: 1.1957\n",
            "Epoch [5/80], Step [4/7840], Loss: 1.2963\n",
            "Epoch [5/80], Step [4/7840], Loss: 1.2366\n",
            "Epoch [5/80], Step [4/7840], Loss: 1.2493\n",
            "Epoch [5/80], Step [4/7840], Loss: 1.2717\n",
            "Epoch [5/80], Step [4/7840], Loss: 1.2959\n",
            "Epoch [5/80], Step [4/7840], Loss: 1.2648\n",
            "Epoch [5/80], Step [4/7840], Loss: 1.2848\n",
            "Epoch [5/80], Step [4/7840], Loss: 1.2647\n",
            "Epoch [6/80], Step [5/7840], Loss: 1.2697\n",
            "Epoch [6/80], Step [5/7840], Loss: 1.2313\n",
            "Epoch [6/80], Step [5/7840], Loss: 1.2260\n",
            "Epoch [6/80], Step [5/7840], Loss: 1.2540\n",
            "Epoch [6/80], Step [5/7840], Loss: 1.2055\n",
            "Epoch [6/80], Step [5/7840], Loss: 1.2283\n",
            "Epoch [6/80], Step [5/7840], Loss: 1.3469\n",
            "Epoch [6/80], Step [5/7840], Loss: 1.2140\n",
            "Epoch [6/80], Step [5/7840], Loss: 1.1874\n",
            "Epoch [7/80], Step [6/7840], Loss: 1.1583\n",
            "Epoch [7/80], Step [6/7840], Loss: 1.2292\n",
            "Epoch [7/80], Step [6/7840], Loss: 1.2239\n",
            "Epoch [7/80], Step [6/7840], Loss: 1.2126\n",
            "Epoch [7/80], Step [6/7840], Loss: 1.2224\n",
            "Epoch [7/80], Step [6/7840], Loss: 1.1762\n",
            "Epoch [7/80], Step [6/7840], Loss: 1.2432\n",
            "Epoch [7/80], Step [6/7840], Loss: 1.1700\n",
            "Epoch [7/80], Step [6/7840], Loss: 1.2130\n",
            "Epoch [8/80], Step [7/7840], Loss: 1.3255\n",
            "Epoch [8/80], Step [7/7840], Loss: 1.1730\n",
            "Epoch [8/80], Step [7/7840], Loss: 1.1251\n",
            "Epoch [8/80], Step [7/7840], Loss: 1.1547\n",
            "Epoch [8/80], Step [7/7840], Loss: 1.2822\n",
            "Epoch [8/80], Step [7/7840], Loss: 1.1221\n",
            "Epoch [8/80], Step [7/7840], Loss: 1.2295\n",
            "Epoch [8/80], Step [7/7840], Loss: 1.0972\n",
            "Epoch [8/80], Step [7/7840], Loss: 1.1759\n",
            "Epoch [9/80], Step [8/7840], Loss: 1.1192\n",
            "Epoch [9/80], Step [8/7840], Loss: 1.1793\n",
            "Epoch [9/80], Step [8/7840], Loss: 1.2253\n",
            "Epoch [9/80], Step [8/7840], Loss: 1.0898\n",
            "Epoch [9/80], Step [8/7840], Loss: 1.0707\n",
            "Epoch [9/80], Step [8/7840], Loss: 1.2038\n",
            "Epoch [9/80], Step [8/7840], Loss: 1.2470\n",
            "Epoch [9/80], Step [8/7840], Loss: 1.1908\n",
            "Epoch [9/80], Step [8/7840], Loss: 1.2996\n",
            "Epoch [10/80], Step [9/7840], Loss: 1.1594\n",
            "Epoch [10/80], Step [9/7840], Loss: 1.2383\n",
            "Epoch [10/80], Step [9/7840], Loss: 1.2045\n",
            "Epoch [10/80], Step [9/7840], Loss: 1.1520\n",
            "Epoch [10/80], Step [9/7840], Loss: 1.1296\n",
            "Epoch [10/80], Step [9/7840], Loss: 1.2253\n",
            "Epoch [10/80], Step [9/7840], Loss: 1.1235\n",
            "Epoch [10/80], Step [9/7840], Loss: 1.2668\n",
            "Epoch [10/80], Step [9/7840], Loss: 1.1492\n",
            "Epoch [11/80], Step [10/7840], Loss: 1.1340\n",
            "Epoch [11/80], Step [10/7840], Loss: 1.2333\n",
            "Epoch [11/80], Step [10/7840], Loss: 1.1650\n",
            "Epoch [11/80], Step [10/7840], Loss: 1.1498\n",
            "Epoch [11/80], Step [10/7840], Loss: 1.1982\n",
            "Epoch [11/80], Step [10/7840], Loss: 1.1786\n",
            "Epoch [11/80], Step [10/7840], Loss: 1.1765\n",
            "Epoch [11/80], Step [10/7840], Loss: 1.1293\n",
            "Epoch [11/80], Step [10/7840], Loss: 1.1599\n",
            "Epoch [12/80], Step [11/7840], Loss: 1.0357\n",
            "Epoch [12/80], Step [11/7840], Loss: 1.1383\n",
            "Epoch [12/80], Step [11/7840], Loss: 1.1791\n",
            "Epoch [12/80], Step [11/7840], Loss: 1.1230\n",
            "Epoch [12/80], Step [11/7840], Loss: 1.1195\n",
            "Epoch [12/80], Step [11/7840], Loss: 1.0836\n",
            "Epoch [12/80], Step [11/7840], Loss: 1.1745\n",
            "Epoch [12/80], Step [11/7840], Loss: 1.0522\n",
            "Epoch [12/80], Step [11/7840], Loss: 1.1900\n",
            "Epoch [13/80], Step [12/7840], Loss: 1.0081\n",
            "Epoch [13/80], Step [12/7840], Loss: 1.1034\n",
            "Epoch [13/80], Step [12/7840], Loss: 1.1927\n",
            "Epoch [13/80], Step [12/7840], Loss: 1.0133\n",
            "Epoch [13/80], Step [12/7840], Loss: 1.1024\n",
            "Epoch [13/80], Step [12/7840], Loss: 1.0866\n",
            "Epoch [13/80], Step [12/7840], Loss: 1.1619\n",
            "Epoch [13/80], Step [12/7840], Loss: 1.1285\n",
            "Epoch [13/80], Step [12/7840], Loss: 1.2409\n",
            "Epoch [14/80], Step [13/7840], Loss: 0.9926\n",
            "Epoch [14/80], Step [13/7840], Loss: 1.0614\n",
            "Epoch [14/80], Step [13/7840], Loss: 1.1126\n",
            "Epoch [14/80], Step [13/7840], Loss: 1.1222\n",
            "Epoch [14/80], Step [13/7840], Loss: 1.1220\n",
            "Epoch [14/80], Step [13/7840], Loss: 1.0916\n",
            "Epoch [14/80], Step [13/7840], Loss: 1.0599\n",
            "Epoch [14/80], Step [13/7840], Loss: 1.1013\n",
            "Epoch [14/80], Step [13/7840], Loss: 1.0650\n",
            "Epoch [15/80], Step [14/7840], Loss: 0.9845\n",
            "Epoch [15/80], Step [14/7840], Loss: 1.0615\n",
            "Epoch [15/80], Step [14/7840], Loss: 1.0909\n",
            "Epoch [15/80], Step [14/7840], Loss: 1.1301\n",
            "Epoch [15/80], Step [14/7840], Loss: 1.0989\n",
            "Epoch [15/80], Step [14/7840], Loss: 1.0836\n",
            "Epoch [15/80], Step [14/7840], Loss: 1.0211\n",
            "Epoch [15/80], Step [14/7840], Loss: 1.1157\n",
            "Epoch [15/80], Step [14/7840], Loss: 1.1444\n",
            "Epoch [16/80], Step [15/7840], Loss: 1.0743\n",
            "Epoch [16/80], Step [15/7840], Loss: 1.0766\n",
            "Epoch [16/80], Step [15/7840], Loss: 1.1053\n",
            "Epoch [16/80], Step [15/7840], Loss: 1.0504\n",
            "Epoch [16/80], Step [15/7840], Loss: 1.0139\n",
            "Epoch [16/80], Step [15/7840], Loss: 1.1388\n",
            "Epoch [16/80], Step [15/7840], Loss: 1.0748\n",
            "Epoch [16/80], Step [15/7840], Loss: 1.1017\n",
            "Epoch [16/80], Step [15/7840], Loss: 1.1555\n",
            "Epoch [17/80], Step [16/7840], Loss: 1.1133\n",
            "Epoch [17/80], Step [16/7840], Loss: 1.1544\n",
            "Epoch [17/80], Step [16/7840], Loss: 1.0275\n",
            "Epoch [17/80], Step [16/7840], Loss: 1.0387\n",
            "Epoch [17/80], Step [16/7840], Loss: 1.0422\n",
            "Epoch [17/80], Step [16/7840], Loss: 1.1134\n",
            "Epoch [17/80], Step [16/7840], Loss: 1.0634\n",
            "Epoch [17/80], Step [16/7840], Loss: 1.0885\n",
            "Epoch [17/80], Step [16/7840], Loss: 1.0576\n",
            "Epoch [18/80], Step [17/7840], Loss: 0.9849\n",
            "Epoch [18/80], Step [17/7840], Loss: 0.9249\n",
            "Epoch [18/80], Step [17/7840], Loss: 0.9572\n",
            "Epoch [18/80], Step [17/7840], Loss: 0.9993\n",
            "Epoch [18/80], Step [17/7840], Loss: 1.0550\n",
            "Epoch [18/80], Step [17/7840], Loss: 1.0232\n",
            "Epoch [18/80], Step [17/7840], Loss: 0.9792\n",
            "Epoch [18/80], Step [17/7840], Loss: 1.0507\n",
            "Epoch [18/80], Step [17/7840], Loss: 1.1073\n",
            "Epoch [19/80], Step [18/7840], Loss: 1.0564\n",
            "Epoch [19/80], Step [18/7840], Loss: 0.9767\n",
            "Epoch [19/80], Step [18/7840], Loss: 1.0798\n",
            "Epoch [19/80], Step [18/7840], Loss: 0.9856\n",
            "Epoch [19/80], Step [18/7840], Loss: 0.9966\n",
            "Epoch [19/80], Step [18/7840], Loss: 0.9718\n",
            "Epoch [19/80], Step [18/7840], Loss: 0.9848\n",
            "Epoch [19/80], Step [18/7840], Loss: 1.0230\n",
            "Epoch [19/80], Step [18/7840], Loss: 1.0635\n",
            "Epoch [20/80], Step [19/7840], Loss: 0.9460\n",
            "Epoch [20/80], Step [19/7840], Loss: 1.0344\n",
            "Epoch [20/80], Step [19/7840], Loss: 0.9338\n",
            "Epoch [20/80], Step [19/7840], Loss: 1.0541\n",
            "Epoch [20/80], Step [19/7840], Loss: 1.0118\n",
            "Epoch [20/80], Step [19/7840], Loss: 0.8859\n",
            "Epoch [20/80], Step [19/7840], Loss: 1.0266\n",
            "Epoch [20/80], Step [19/7840], Loss: 1.0005\n",
            "Epoch [20/80], Step [19/7840], Loss: 0.9390\n",
            "Epoch [21/80], Step [20/7840], Loss: 0.8394\n",
            "Epoch [21/80], Step [20/7840], Loss: 1.0463\n",
            "Epoch [21/80], Step [20/7840], Loss: 0.9728\n",
            "Epoch [21/80], Step [20/7840], Loss: 0.9786\n",
            "Epoch [21/80], Step [20/7840], Loss: 1.0330\n",
            "Epoch [21/80], Step [20/7840], Loss: 1.0222\n",
            "Epoch [21/80], Step [20/7840], Loss: 1.0138\n",
            "Epoch [21/80], Step [20/7840], Loss: 1.0080\n",
            "Epoch [21/80], Step [20/7840], Loss: 1.0025\n",
            "Epoch [22/80], Step [21/7840], Loss: 0.8423\n",
            "Epoch [22/80], Step [21/7840], Loss: 0.9253\n",
            "Epoch [22/80], Step [21/7840], Loss: 1.0477\n",
            "Epoch [22/80], Step [21/7840], Loss: 0.9082\n",
            "Epoch [22/80], Step [21/7840], Loss: 0.9787\n",
            "Epoch [22/80], Step [21/7840], Loss: 1.0398\n",
            "Epoch [22/80], Step [21/7840], Loss: 0.9611\n",
            "Epoch [22/80], Step [21/7840], Loss: 0.9378\n",
            "Epoch [22/80], Step [21/7840], Loss: 0.9677\n",
            "Epoch [23/80], Step [22/7840], Loss: 0.8126\n",
            "Epoch [23/80], Step [22/7840], Loss: 0.9335\n",
            "Epoch [23/80], Step [22/7840], Loss: 0.9132\n",
            "Epoch [23/80], Step [22/7840], Loss: 0.9867\n",
            "Epoch [23/80], Step [22/7840], Loss: 0.8502\n",
            "Epoch [23/80], Step [22/7840], Loss: 0.9889\n",
            "Epoch [23/80], Step [22/7840], Loss: 0.9487\n",
            "Epoch [23/80], Step [22/7840], Loss: 1.0046\n",
            "Epoch [23/80], Step [22/7840], Loss: 0.9544\n",
            "Epoch [24/80], Step [23/7840], Loss: 0.9539\n",
            "Epoch [24/80], Step [23/7840], Loss: 0.8680\n",
            "Epoch [24/80], Step [23/7840], Loss: 0.9791\n",
            "Epoch [24/80], Step [23/7840], Loss: 0.9523\n",
            "Epoch [24/80], Step [23/7840], Loss: 0.8160\n",
            "Epoch [24/80], Step [23/7840], Loss: 1.0077\n",
            "Epoch [24/80], Step [23/7840], Loss: 0.8596\n",
            "Epoch [24/80], Step [23/7840], Loss: 0.8934\n",
            "Epoch [24/80], Step [23/7840], Loss: 0.9419\n",
            "Epoch [25/80], Step [24/7840], Loss: 0.7169\n",
            "Epoch [25/80], Step [24/7840], Loss: 0.8495\n",
            "Epoch [25/80], Step [24/7840], Loss: 0.9020\n",
            "Epoch [25/80], Step [24/7840], Loss: 0.8540\n",
            "Epoch [25/80], Step [24/7840], Loss: 0.8348\n",
            "Epoch [25/80], Step [24/7840], Loss: 1.0045\n",
            "Epoch [25/80], Step [24/7840], Loss: 0.8778\n",
            "Epoch [25/80], Step [24/7840], Loss: 0.8511\n",
            "Epoch [25/80], Step [24/7840], Loss: 0.8059\n",
            "Epoch [26/80], Step [25/7840], Loss: 0.9165\n",
            "Epoch [26/80], Step [25/7840], Loss: 0.7706\n",
            "Epoch [26/80], Step [25/7840], Loss: 0.8733\n",
            "Epoch [26/80], Step [25/7840], Loss: 0.8476\n",
            "Epoch [26/80], Step [25/7840], Loss: 0.7902\n",
            "Epoch [26/80], Step [25/7840], Loss: 0.8549\n",
            "Epoch [26/80], Step [25/7840], Loss: 0.8424\n",
            "Epoch [26/80], Step [25/7840], Loss: 0.8813\n",
            "Epoch [26/80], Step [25/7840], Loss: 0.8828\n",
            "Epoch [27/80], Step [26/7840], Loss: 0.7035\n",
            "Epoch [27/80], Step [26/7840], Loss: 0.8047\n",
            "Epoch [27/80], Step [26/7840], Loss: 0.7952\n",
            "Epoch [27/80], Step [26/7840], Loss: 0.8388\n",
            "Epoch [27/80], Step [26/7840], Loss: 0.7211\n",
            "Epoch [27/80], Step [26/7840], Loss: 0.8470\n",
            "Epoch [27/80], Step [26/7840], Loss: 0.7249\n",
            "Epoch [27/80], Step [26/7840], Loss: 0.7542\n",
            "Epoch [27/80], Step [26/7840], Loss: 0.8256\n",
            "Epoch [28/80], Step [27/7840], Loss: 0.6720\n",
            "Epoch [28/80], Step [27/7840], Loss: 0.7865\n",
            "Epoch [28/80], Step [27/7840], Loss: 0.8219\n",
            "Epoch [28/80], Step [27/7840], Loss: 0.7892\n",
            "Epoch [28/80], Step [27/7840], Loss: 0.6930\n",
            "Epoch [28/80], Step [27/7840], Loss: 0.7412\n",
            "Epoch [28/80], Step [27/7840], Loss: 0.7465\n",
            "Epoch [28/80], Step [27/7840], Loss: 0.8163\n",
            "Epoch [28/80], Step [27/7840], Loss: 0.8650\n",
            "Epoch [29/80], Step [28/7840], Loss: 0.7412\n",
            "Epoch [29/80], Step [28/7840], Loss: 0.8430\n",
            "Epoch [29/80], Step [28/7840], Loss: 0.7101\n",
            "Epoch [29/80], Step [28/7840], Loss: 0.5590\n",
            "Epoch [29/80], Step [28/7840], Loss: 0.6997\n",
            "Epoch [29/80], Step [28/7840], Loss: 0.7454\n",
            "Epoch [29/80], Step [28/7840], Loss: 0.7759\n",
            "Epoch [29/80], Step [28/7840], Loss: 0.7562\n",
            "Epoch [29/80], Step [28/7840], Loss: 0.6874\n",
            "Epoch [30/80], Step [29/7840], Loss: 0.6174\n",
            "Epoch [30/80], Step [29/7840], Loss: 0.5887\n",
            "Epoch [30/80], Step [29/7840], Loss: 0.7783\n",
            "Epoch [30/80], Step [29/7840], Loss: 0.5038\n",
            "Epoch [30/80], Step [29/7840], Loss: 0.5646\n",
            "Epoch [30/80], Step [29/7840], Loss: 0.5884\n",
            "Epoch [30/80], Step [29/7840], Loss: 0.8369\n",
            "Epoch [30/80], Step [29/7840], Loss: 0.6945\n",
            "Epoch [30/80], Step [29/7840], Loss: 0.6419\n",
            "Epoch [31/80], Step [30/7840], Loss: 0.4766\n",
            "Epoch [31/80], Step [30/7840], Loss: 0.5510\n",
            "Epoch [31/80], Step [30/7840], Loss: 0.5479\n",
            "Epoch [31/80], Step [30/7840], Loss: 0.5615\n",
            "Epoch [31/80], Step [30/7840], Loss: 0.6712\n",
            "Epoch [31/80], Step [30/7840], Loss: 0.7374\n",
            "Epoch [31/80], Step [30/7840], Loss: 0.6725\n",
            "Epoch [31/80], Step [30/7840], Loss: 0.5601\n",
            "Epoch [31/80], Step [30/7840], Loss: 0.7411\n",
            "Epoch [32/80], Step [31/7840], Loss: 0.5475\n",
            "Epoch [32/80], Step [31/7840], Loss: 0.6434\n",
            "Epoch [32/80], Step [31/7840], Loss: 0.6067\n",
            "Epoch [32/80], Step [31/7840], Loss: 0.6577\n",
            "Epoch [32/80], Step [31/7840], Loss: 0.6461\n",
            "Epoch [32/80], Step [31/7840], Loss: 0.5337\n",
            "Epoch [32/80], Step [31/7840], Loss: 0.6360\n",
            "Epoch [32/80], Step [31/7840], Loss: 0.6096\n",
            "Epoch [32/80], Step [31/7840], Loss: 0.6809\n",
            "Epoch [33/80], Step [32/7840], Loss: 0.4215\n",
            "Epoch [33/80], Step [32/7840], Loss: 0.5158\n",
            "Epoch [33/80], Step [32/7840], Loss: 0.5765\n",
            "Epoch [33/80], Step [32/7840], Loss: 0.5816\n",
            "Epoch [33/80], Step [32/7840], Loss: 0.5017\n",
            "Epoch [33/80], Step [32/7840], Loss: 0.6135\n",
            "Epoch [33/80], Step [32/7840], Loss: 0.5846\n",
            "Epoch [33/80], Step [32/7840], Loss: 0.5591\n",
            "Epoch [33/80], Step [32/7840], Loss: 0.5247\n",
            "Epoch [34/80], Step [33/7840], Loss: 0.5060\n",
            "Epoch [34/80], Step [33/7840], Loss: 0.4635\n",
            "Epoch [34/80], Step [33/7840], Loss: 0.4716\n",
            "Epoch [34/80], Step [33/7840], Loss: 0.4510\n",
            "Epoch [34/80], Step [33/7840], Loss: 0.4348\n",
            "Epoch [34/80], Step [33/7840], Loss: 0.5097\n",
            "Epoch [34/80], Step [33/7840], Loss: 0.5528\n",
            "Epoch [34/80], Step [33/7840], Loss: 0.6308\n",
            "Epoch [34/80], Step [33/7840], Loss: 0.4936\n",
            "Epoch [35/80], Step [34/7840], Loss: 0.4357\n",
            "Epoch [35/80], Step [34/7840], Loss: 0.5013\n",
            "Epoch [35/80], Step [34/7840], Loss: 0.4070\n",
            "Epoch [35/80], Step [34/7840], Loss: 0.5572\n",
            "Epoch [35/80], Step [34/7840], Loss: 0.5733\n",
            "Epoch [35/80], Step [34/7840], Loss: 0.5340\n",
            "Epoch [35/80], Step [34/7840], Loss: 0.4702\n",
            "Epoch [35/80], Step [34/7840], Loss: 0.5196\n",
            "Epoch [35/80], Step [34/7840], Loss: 0.4207\n",
            "Epoch [36/80], Step [35/7840], Loss: 0.4041\n",
            "Epoch [36/80], Step [35/7840], Loss: 0.4415\n",
            "Epoch [36/80], Step [35/7840], Loss: 0.4012\n",
            "Epoch [36/80], Step [35/7840], Loss: 0.3445\n",
            "Epoch [36/80], Step [35/7840], Loss: 0.4057\n",
            "Epoch [36/80], Step [35/7840], Loss: 0.3792\n",
            "Epoch [36/80], Step [35/7840], Loss: 0.4111\n",
            "Epoch [36/80], Step [35/7840], Loss: 0.4497\n",
            "Epoch [36/80], Step [35/7840], Loss: 0.4339\n",
            "Epoch [37/80], Step [36/7840], Loss: 0.4519\n",
            "Epoch [37/80], Step [36/7840], Loss: 0.3966\n",
            "Epoch [37/80], Step [36/7840], Loss: 0.4045\n",
            "Epoch [37/80], Step [36/7840], Loss: 0.4230\n",
            "Epoch [37/80], Step [36/7840], Loss: 0.4475\n",
            "Epoch [37/80], Step [36/7840], Loss: 0.5563\n",
            "Epoch [37/80], Step [36/7840], Loss: 0.4502\n",
            "Epoch [37/80], Step [36/7840], Loss: 0.4956\n",
            "Epoch [37/80], Step [36/7840], Loss: 0.4342\n",
            "Epoch [38/80], Step [37/7840], Loss: 0.3887\n",
            "Epoch [38/80], Step [37/7840], Loss: 0.5169\n",
            "Epoch [38/80], Step [37/7840], Loss: 0.4082\n",
            "Epoch [38/80], Step [37/7840], Loss: 0.3583\n",
            "Epoch [38/80], Step [37/7840], Loss: 0.5039\n",
            "Epoch [38/80], Step [37/7840], Loss: 0.4389\n",
            "Epoch [38/80], Step [37/7840], Loss: 0.4691\n",
            "Epoch [38/80], Step [37/7840], Loss: 0.3977\n",
            "Epoch [38/80], Step [37/7840], Loss: 0.3733\n",
            "Epoch [39/80], Step [38/7840], Loss: 0.3560\n",
            "Epoch [39/80], Step [38/7840], Loss: 0.3229\n",
            "Epoch [39/80], Step [38/7840], Loss: 0.3239\n",
            "Epoch [39/80], Step [38/7840], Loss: 0.2980\n",
            "Epoch [39/80], Step [38/7840], Loss: 0.2935\n",
            "Epoch [39/80], Step [38/7840], Loss: 0.3365\n",
            "Epoch [39/80], Step [38/7840], Loss: 0.3784\n",
            "Epoch [39/80], Step [38/7840], Loss: 0.4487\n",
            "Epoch [39/80], Step [38/7840], Loss: 0.3202\n",
            "Epoch [40/80], Step [39/7840], Loss: 0.2899\n",
            "Epoch [40/80], Step [39/7840], Loss: 0.2909\n",
            "Epoch [40/80], Step [39/7840], Loss: 0.2810\n",
            "Epoch [40/80], Step [39/7840], Loss: 0.3209\n",
            "Epoch [40/80], Step [39/7840], Loss: 0.3746\n",
            "Epoch [40/80], Step [39/7840], Loss: 0.4276\n",
            "Epoch [40/80], Step [39/7840], Loss: 0.3028\n",
            "Epoch [40/80], Step [39/7840], Loss: 0.2617\n",
            "Epoch [40/80], Step [39/7840], Loss: 0.3494\n",
            "Epoch [41/80], Step [40/7840], Loss: 0.3010\n",
            "Epoch [41/80], Step [40/7840], Loss: 0.2653\n",
            "Epoch [41/80], Step [40/7840], Loss: 0.2846\n",
            "Epoch [41/80], Step [40/7840], Loss: 0.3579\n",
            "Epoch [41/80], Step [40/7840], Loss: 0.4346\n",
            "Epoch [41/80], Step [40/7840], Loss: 0.3740\n",
            "Epoch [41/80], Step [40/7840], Loss: 0.4062\n",
            "Epoch [41/80], Step [40/7840], Loss: 0.2856\n",
            "Epoch [41/80], Step [40/7840], Loss: 0.2516\n",
            "Epoch [42/80], Step [41/7840], Loss: 0.3283\n",
            "Epoch [42/80], Step [41/7840], Loss: 0.2235\n",
            "Epoch [42/80], Step [41/7840], Loss: 0.3515\n",
            "Epoch [42/80], Step [41/7840], Loss: 0.2710\n",
            "Epoch [42/80], Step [41/7840], Loss: 0.3663\n",
            "Epoch [42/80], Step [41/7840], Loss: 0.4122\n",
            "Epoch [42/80], Step [41/7840], Loss: 0.3627\n",
            "Epoch [42/80], Step [41/7840], Loss: 0.2889\n",
            "Epoch [42/80], Step [41/7840], Loss: 0.3647\n",
            "Epoch [43/80], Step [42/7840], Loss: 0.1834\n",
            "Epoch [43/80], Step [42/7840], Loss: 0.2154\n",
            "Epoch [43/80], Step [42/7840], Loss: 0.2594\n",
            "Epoch [43/80], Step [42/7840], Loss: 0.2921\n",
            "Epoch [43/80], Step [42/7840], Loss: 0.3371\n",
            "Epoch [43/80], Step [42/7840], Loss: 0.2309\n",
            "Epoch [43/80], Step [42/7840], Loss: 0.2673\n",
            "Epoch [43/80], Step [42/7840], Loss: 0.2750\n",
            "Epoch [43/80], Step [42/7840], Loss: 0.2837\n",
            "Epoch [44/80], Step [43/7840], Loss: 0.2538\n",
            "Epoch [44/80], Step [43/7840], Loss: 0.1592\n",
            "Epoch [44/80], Step [43/7840], Loss: 0.2792\n",
            "Epoch [44/80], Step [43/7840], Loss: 0.2806\n",
            "Epoch [44/80], Step [43/7840], Loss: 0.2484\n",
            "Epoch [44/80], Step [43/7840], Loss: 0.2415\n",
            "Epoch [44/80], Step [43/7840], Loss: 0.2584\n",
            "Epoch [44/80], Step [43/7840], Loss: 0.2414\n",
            "Epoch [44/80], Step [43/7840], Loss: 0.2943\n",
            "Epoch [45/80], Step [44/7840], Loss: 0.2213\n",
            "Epoch [45/80], Step [44/7840], Loss: 0.2325\n",
            "Epoch [45/80], Step [44/7840], Loss: 0.1644\n",
            "Epoch [45/80], Step [44/7840], Loss: 0.2167\n",
            "Epoch [45/80], Step [44/7840], Loss: 0.2333\n",
            "Epoch [45/80], Step [44/7840], Loss: 0.2813\n",
            "Epoch [45/80], Step [44/7840], Loss: 0.2365\n",
            "Epoch [45/80], Step [44/7840], Loss: 0.1994\n",
            "Epoch [45/80], Step [44/7840], Loss: 0.2226\n",
            "Epoch [46/80], Step [45/7840], Loss: 0.1872\n",
            "Epoch [46/80], Step [45/7840], Loss: 0.2448\n",
            "Epoch [46/80], Step [45/7840], Loss: 0.2696\n",
            "Epoch [46/80], Step [45/7840], Loss: 0.1987\n",
            "Epoch [46/80], Step [45/7840], Loss: 0.2239\n",
            "Epoch [46/80], Step [45/7840], Loss: 0.2754\n",
            "Epoch [46/80], Step [45/7840], Loss: 0.2080\n",
            "Epoch [46/80], Step [45/7840], Loss: 0.2465\n",
            "Epoch [46/80], Step [45/7840], Loss: 0.2574\n",
            "Epoch [47/80], Step [46/7840], Loss: 0.1320\n",
            "Epoch [47/80], Step [46/7840], Loss: 0.2082\n",
            "Epoch [47/80], Step [46/7840], Loss: 0.1776\n",
            "Epoch [47/80], Step [46/7840], Loss: 0.2408\n",
            "Epoch [47/80], Step [46/7840], Loss: 0.2184\n",
            "Epoch [47/80], Step [46/7840], Loss: 0.2869\n",
            "Epoch [47/80], Step [46/7840], Loss: 0.2920\n",
            "Epoch [47/80], Step [46/7840], Loss: 0.2805\n",
            "Epoch [47/80], Step [46/7840], Loss: 0.2420\n",
            "Epoch [48/80], Step [47/7840], Loss: 0.2264\n",
            "Epoch [48/80], Step [47/7840], Loss: 0.2039\n",
            "Epoch [48/80], Step [47/7840], Loss: 0.1778\n",
            "Epoch [48/80], Step [47/7840], Loss: 0.2410\n",
            "Epoch [48/80], Step [47/7840], Loss: 0.1797\n",
            "Epoch [48/80], Step [47/7840], Loss: 0.1930\n",
            "Epoch [48/80], Step [47/7840], Loss: 0.2162\n",
            "Epoch [48/80], Step [47/7840], Loss: 0.2334\n",
            "Epoch [48/80], Step [47/7840], Loss: 0.1961\n",
            "Epoch [49/80], Step [48/7840], Loss: 0.1549\n",
            "Epoch [49/80], Step [48/7840], Loss: 0.1874\n",
            "Epoch [49/80], Step [48/7840], Loss: 0.1622\n",
            "Epoch [49/80], Step [48/7840], Loss: 0.1524\n",
            "Epoch [49/80], Step [48/7840], Loss: 0.2479\n",
            "Epoch [49/80], Step [48/7840], Loss: 0.2905\n",
            "Epoch [49/80], Step [48/7840], Loss: 0.1673\n",
            "Epoch [49/80], Step [48/7840], Loss: 0.2037\n",
            "Epoch [49/80], Step [48/7840], Loss: 0.1558\n",
            "Epoch [50/80], Step [49/7840], Loss: 0.1983\n",
            "Epoch [50/80], Step [49/7840], Loss: 0.1613\n",
            "Epoch [50/80], Step [49/7840], Loss: 0.1468\n",
            "Epoch [50/80], Step [49/7840], Loss: 0.1479\n",
            "Epoch [50/80], Step [49/7840], Loss: 0.1536\n",
            "Epoch [50/80], Step [49/7840], Loss: 0.2722\n",
            "Epoch [50/80], Step [49/7840], Loss: 0.2058\n",
            "Epoch [50/80], Step [49/7840], Loss: 0.2055\n",
            "Epoch [50/80], Step [49/7840], Loss: 0.1947\n",
            "Epoch [51/80], Step [50/7840], Loss: 0.1242\n",
            "Epoch [51/80], Step [50/7840], Loss: 0.1577\n",
            "Epoch [51/80], Step [50/7840], Loss: 0.1533\n",
            "Epoch [51/80], Step [50/7840], Loss: 0.2077\n",
            "Epoch [51/80], Step [50/7840], Loss: 0.1863\n",
            "Epoch [51/80], Step [50/7840], Loss: 0.2130\n",
            "Epoch [51/80], Step [50/7840], Loss: 0.1417\n",
            "Epoch [51/80], Step [50/7840], Loss: 0.1853\n",
            "Epoch [51/80], Step [50/7840], Loss: 0.1803\n",
            "Epoch [52/80], Step [51/7840], Loss: 0.1439\n",
            "Epoch [52/80], Step [51/7840], Loss: 0.1541\n",
            "Epoch [52/80], Step [51/7840], Loss: 0.2497\n",
            "Epoch [52/80], Step [51/7840], Loss: 0.1422\n",
            "Epoch [52/80], Step [51/7840], Loss: 0.1648\n",
            "Epoch [52/80], Step [51/7840], Loss: 0.1950\n",
            "Epoch [52/80], Step [51/7840], Loss: 0.1180\n",
            "Epoch [52/80], Step [51/7840], Loss: 0.1312\n",
            "Epoch [52/80], Step [51/7840], Loss: 0.1475\n",
            "Epoch [53/80], Step [52/7840], Loss: 0.2225\n",
            "Epoch [53/80], Step [52/7840], Loss: 0.2018\n",
            "Epoch [53/80], Step [52/7840], Loss: 0.1477\n",
            "Epoch [53/80], Step [52/7840], Loss: 0.1351\n",
            "Epoch [53/80], Step [52/7840], Loss: 0.1821\n",
            "Epoch [53/80], Step [52/7840], Loss: 0.1771\n",
            "Epoch [53/80], Step [52/7840], Loss: 0.1530\n",
            "Epoch [53/80], Step [52/7840], Loss: 0.1806\n",
            "Epoch [53/80], Step [52/7840], Loss: 0.1841\n",
            "Epoch [54/80], Step [53/7840], Loss: 0.1198\n",
            "Epoch [54/80], Step [53/7840], Loss: 0.1318\n",
            "Epoch [54/80], Step [53/7840], Loss: 0.1564\n",
            "Epoch [54/80], Step [53/7840], Loss: 0.1441\n",
            "Epoch [54/80], Step [53/7840], Loss: 0.1318\n",
            "Epoch [54/80], Step [53/7840], Loss: 0.0871\n",
            "Epoch [54/80], Step [53/7840], Loss: 0.1121\n",
            "Epoch [54/80], Step [53/7840], Loss: 0.1014\n",
            "Epoch [54/80], Step [53/7840], Loss: 0.1303\n",
            "Epoch [55/80], Step [54/7840], Loss: 0.1255\n",
            "Epoch [55/80], Step [54/7840], Loss: 0.1708\n",
            "Epoch [55/80], Step [54/7840], Loss: 0.2513\n",
            "Epoch [55/80], Step [54/7840], Loss: 0.1202\n",
            "Epoch [55/80], Step [54/7840], Loss: 0.1547\n",
            "Epoch [55/80], Step [54/7840], Loss: 0.1016\n",
            "Epoch [55/80], Step [54/7840], Loss: 0.1463\n",
            "Epoch [55/80], Step [54/7840], Loss: 0.1402\n",
            "Epoch [55/80], Step [54/7840], Loss: 0.1864\n",
            "Epoch [56/80], Step [55/7840], Loss: 0.1351\n",
            "Epoch [56/80], Step [55/7840], Loss: 0.1264\n",
            "Epoch [56/80], Step [55/7840], Loss: 0.1330\n",
            "Epoch [56/80], Step [55/7840], Loss: 0.1434\n",
            "Epoch [56/80], Step [55/7840], Loss: 0.1453\n",
            "Epoch [56/80], Step [55/7840], Loss: 0.2078\n",
            "Epoch [56/80], Step [55/7840], Loss: 0.1657\n",
            "Epoch [56/80], Step [55/7840], Loss: 0.1540\n",
            "Epoch [56/80], Step [55/7840], Loss: 0.1818\n",
            "Epoch [57/80], Step [56/7840], Loss: 0.0993\n",
            "Epoch [57/80], Step [56/7840], Loss: 0.1291\n",
            "Epoch [57/80], Step [56/7840], Loss: 0.1288\n",
            "Epoch [57/80], Step [56/7840], Loss: 0.1411\n",
            "Epoch [57/80], Step [56/7840], Loss: 0.1752\n",
            "Epoch [57/80], Step [56/7840], Loss: 0.1124\n",
            "Epoch [57/80], Step [56/7840], Loss: 0.1153\n",
            "Epoch [57/80], Step [56/7840], Loss: 0.1684\n",
            "Epoch [57/80], Step [56/7840], Loss: 0.1189\n",
            "Epoch [58/80], Step [57/7840], Loss: 0.1359\n",
            "Epoch [58/80], Step [57/7840], Loss: 0.1368\n",
            "Epoch [58/80], Step [57/7840], Loss: 0.1813\n",
            "Epoch [58/80], Step [57/7840], Loss: 0.1082\n",
            "Epoch [58/80], Step [57/7840], Loss: 0.1644\n",
            "Epoch [58/80], Step [57/7840], Loss: 0.0657\n",
            "Epoch [58/80], Step [57/7840], Loss: 0.1430\n",
            "Epoch [58/80], Step [57/7840], Loss: 0.0957\n",
            "Epoch [58/80], Step [57/7840], Loss: 0.1036\n",
            "Epoch [59/80], Step [58/7840], Loss: 0.2013\n",
            "Epoch [59/80], Step [58/7840], Loss: 0.1985\n",
            "Epoch [59/80], Step [58/7840], Loss: 0.0628\n",
            "Epoch [59/80], Step [58/7840], Loss: 0.1210\n",
            "Epoch [59/80], Step [58/7840], Loss: 0.1473\n",
            "Epoch [59/80], Step [58/7840], Loss: 0.1057\n",
            "Epoch [59/80], Step [58/7840], Loss: 0.1838\n",
            "Epoch [59/80], Step [58/7840], Loss: 0.0988\n",
            "Epoch [59/80], Step [58/7840], Loss: 0.1490\n",
            "Epoch [60/80], Step [59/7840], Loss: 0.0914\n",
            "Epoch [60/80], Step [59/7840], Loss: 0.0919\n",
            "Epoch [60/80], Step [59/7840], Loss: 0.1351\n",
            "Epoch [60/80], Step [59/7840], Loss: 0.1730\n",
            "Epoch [60/80], Step [59/7840], Loss: 0.1060\n",
            "Epoch [60/80], Step [59/7840], Loss: 0.2242\n",
            "Epoch [60/80], Step [59/7840], Loss: 0.1186\n",
            "Epoch [60/80], Step [59/7840], Loss: 0.1187\n",
            "Epoch [60/80], Step [59/7840], Loss: 0.0985\n",
            "Epoch [61/80], Step [60/7840], Loss: 0.1039\n",
            "Epoch [61/80], Step [60/7840], Loss: 0.1505\n",
            "Epoch [61/80], Step [60/7840], Loss: 0.0850\n",
            "Epoch [61/80], Step [60/7840], Loss: 0.1178\n",
            "Epoch [61/80], Step [60/7840], Loss: 0.1170\n",
            "Epoch [61/80], Step [60/7840], Loss: 0.1177\n",
            "Epoch [61/80], Step [60/7840], Loss: 0.1645\n",
            "Epoch [61/80], Step [60/7840], Loss: 0.1240\n",
            "Epoch [61/80], Step [60/7840], Loss: 0.1527\n",
            "Epoch [62/80], Step [61/7840], Loss: 0.1200\n",
            "Epoch [62/80], Step [61/7840], Loss: 0.0943\n",
            "Epoch [62/80], Step [61/7840], Loss: 0.1077\n",
            "Epoch [62/80], Step [61/7840], Loss: 0.1387\n",
            "Epoch [62/80], Step [61/7840], Loss: 0.0778\n",
            "Epoch [62/80], Step [61/7840], Loss: 0.0850\n",
            "Epoch [62/80], Step [61/7840], Loss: 0.1252\n",
            "Epoch [62/80], Step [61/7840], Loss: 0.0933\n",
            "Epoch [62/80], Step [61/7840], Loss: 0.1664\n",
            "Epoch [63/80], Step [62/7840], Loss: 0.1446\n",
            "Epoch [63/80], Step [62/7840], Loss: 0.0939\n",
            "Epoch [63/80], Step [62/7840], Loss: 0.1907\n",
            "Epoch [63/80], Step [62/7840], Loss: 0.1184\n",
            "Epoch [63/80], Step [62/7840], Loss: 0.1457\n",
            "Epoch [63/80], Step [62/7840], Loss: 0.0948\n",
            "Epoch [63/80], Step [62/7840], Loss: 0.1471\n",
            "Epoch [63/80], Step [62/7840], Loss: 0.1446\n",
            "Epoch [63/80], Step [62/7840], Loss: 0.0839\n",
            "Epoch [64/80], Step [63/7840], Loss: 0.1454\n",
            "Epoch [64/80], Step [63/7840], Loss: 0.1086\n",
            "Epoch [64/80], Step [63/7840], Loss: 0.0707\n",
            "Epoch [64/80], Step [63/7840], Loss: 0.0830\n",
            "Epoch [64/80], Step [63/7840], Loss: 0.1182\n",
            "Epoch [64/80], Step [63/7840], Loss: 0.0651\n",
            "Epoch [64/80], Step [63/7840], Loss: 0.1369\n",
            "Epoch [64/80], Step [63/7840], Loss: 0.1298\n",
            "Epoch [64/80], Step [63/7840], Loss: 0.1369\n",
            "Epoch [65/80], Step [64/7840], Loss: 0.0914\n",
            "Epoch [65/80], Step [64/7840], Loss: 0.0744\n",
            "Epoch [65/80], Step [64/7840], Loss: 0.1275\n",
            "Epoch [65/80], Step [64/7840], Loss: 0.0846\n",
            "Epoch [65/80], Step [64/7840], Loss: 0.1008\n",
            "Epoch [65/80], Step [64/7840], Loss: 0.1571\n",
            "Epoch [65/80], Step [64/7840], Loss: 0.1125\n",
            "Epoch [65/80], Step [64/7840], Loss: 0.0825\n",
            "Epoch [65/80], Step [64/7840], Loss: 0.1077\n",
            "Epoch [66/80], Step [65/7840], Loss: 0.1485\n",
            "Epoch [66/80], Step [65/7840], Loss: 0.1005\n",
            "Epoch [66/80], Step [65/7840], Loss: 0.1069\n",
            "Epoch [66/80], Step [65/7840], Loss: 0.0932\n",
            "Epoch [66/80], Step [65/7840], Loss: 0.1386\n",
            "Epoch [66/80], Step [65/7840], Loss: 0.1607\n",
            "Epoch [66/80], Step [65/7840], Loss: 0.1486\n",
            "Epoch [66/80], Step [65/7840], Loss: 0.1129\n",
            "Epoch [66/80], Step [65/7840], Loss: 0.0803\n",
            "Epoch [67/80], Step [66/7840], Loss: 0.0591\n",
            "Epoch [67/80], Step [66/7840], Loss: 0.0651\n",
            "Epoch [67/80], Step [66/7840], Loss: 0.0875\n",
            "Epoch [67/80], Step [66/7840], Loss: 0.0872\n",
            "Epoch [67/80], Step [66/7840], Loss: 0.1215\n",
            "Epoch [67/80], Step [66/7840], Loss: 0.0970\n",
            "Epoch [67/80], Step [66/7840], Loss: 0.0972\n",
            "Epoch [67/80], Step [66/7840], Loss: 0.1079\n",
            "Epoch [67/80], Step [66/7840], Loss: 0.0847\n",
            "Epoch [68/80], Step [67/7840], Loss: 0.1617\n",
            "Epoch [68/80], Step [67/7840], Loss: 0.1210\n",
            "Epoch [68/80], Step [67/7840], Loss: 0.1369\n",
            "Epoch [68/80], Step [67/7840], Loss: 0.1359\n",
            "Epoch [68/80], Step [67/7840], Loss: 0.0872\n",
            "Epoch [68/80], Step [67/7840], Loss: 0.1208\n",
            "Epoch [68/80], Step [67/7840], Loss: 0.1064\n",
            "Epoch [68/80], Step [67/7840], Loss: 0.0836\n",
            "Epoch [68/80], Step [67/7840], Loss: 0.0884\n",
            "Epoch [69/80], Step [68/7840], Loss: 0.0917\n",
            "Epoch [69/80], Step [68/7840], Loss: 0.1763\n",
            "Epoch [69/80], Step [68/7840], Loss: 0.0602\n",
            "Epoch [69/80], Step [68/7840], Loss: 0.1481\n",
            "Epoch [69/80], Step [68/7840], Loss: 0.0943\n",
            "Epoch [69/80], Step [68/7840], Loss: 0.0872\n",
            "Epoch [69/80], Step [68/7840], Loss: 0.0953\n",
            "Epoch [69/80], Step [68/7840], Loss: 0.0879\n",
            "Epoch [69/80], Step [68/7840], Loss: 0.0588\n",
            "Epoch [70/80], Step [69/7840], Loss: 0.1127\n",
            "Epoch [70/80], Step [69/7840], Loss: 0.0990\n",
            "Epoch [70/80], Step [69/7840], Loss: 0.0544\n",
            "Epoch [70/80], Step [69/7840], Loss: 0.1014\n",
            "Epoch [70/80], Step [69/7840], Loss: 0.1314\n",
            "Epoch [70/80], Step [69/7840], Loss: 0.1144\n",
            "Epoch [70/80], Step [69/7840], Loss: 0.1265\n",
            "Epoch [70/80], Step [69/7840], Loss: 0.1389\n",
            "Epoch [70/80], Step [69/7840], Loss: 0.1207\n",
            "Epoch [71/80], Step [70/7840], Loss: 0.1047\n",
            "Epoch [71/80], Step [70/7840], Loss: 0.0692\n",
            "Epoch [71/80], Step [70/7840], Loss: 0.0720\n",
            "Epoch [71/80], Step [70/7840], Loss: 0.1765\n",
            "Epoch [71/80], Step [70/7840], Loss: 0.1163\n",
            "Epoch [71/80], Step [70/7840], Loss: 0.0881\n",
            "Epoch [71/80], Step [70/7840], Loss: 0.1446\n",
            "Epoch [71/80], Step [70/7840], Loss: 0.1887\n",
            "Epoch [71/80], Step [70/7840], Loss: 0.1310\n",
            "Epoch [72/80], Step [71/7840], Loss: 0.0950\n",
            "Epoch [72/80], Step [71/7840], Loss: 0.0839\n",
            "Epoch [72/80], Step [71/7840], Loss: 0.0727\n",
            "Epoch [72/80], Step [71/7840], Loss: 0.0744\n",
            "Epoch [72/80], Step [71/7840], Loss: 0.1398\n",
            "Epoch [72/80], Step [71/7840], Loss: 0.1049\n",
            "Epoch [72/80], Step [71/7840], Loss: 0.1013\n",
            "Epoch [72/80], Step [71/7840], Loss: 0.0851\n",
            "Epoch [72/80], Step [71/7840], Loss: 0.0704\n",
            "Epoch [73/80], Step [72/7840], Loss: 0.0827\n",
            "Epoch [73/80], Step [72/7840], Loss: 0.0782\n",
            "Epoch [73/80], Step [72/7840], Loss: 0.1644\n",
            "Epoch [73/80], Step [72/7840], Loss: 0.0884\n",
            "Epoch [73/80], Step [72/7840], Loss: 0.1006\n",
            "Epoch [73/80], Step [72/7840], Loss: 0.0662\n",
            "Epoch [73/80], Step [72/7840], Loss: 0.0760\n",
            "Epoch [73/80], Step [72/7840], Loss: 0.1094\n",
            "Epoch [73/80], Step [72/7840], Loss: 0.1113\n",
            "Epoch [74/80], Step [73/7840], Loss: 0.1009\n",
            "Epoch [74/80], Step [73/7840], Loss: 0.1184\n",
            "Epoch [74/80], Step [73/7840], Loss: 0.1067\n",
            "Epoch [74/80], Step [73/7840], Loss: 0.0406\n",
            "Epoch [74/80], Step [73/7840], Loss: 0.0957\n",
            "Epoch [74/80], Step [73/7840], Loss: 0.0863\n",
            "Epoch [74/80], Step [73/7840], Loss: 0.0763\n",
            "Epoch [74/80], Step [73/7840], Loss: 0.0836\n",
            "Epoch [74/80], Step [73/7840], Loss: 0.1080\n",
            "Epoch [75/80], Step [74/7840], Loss: 0.0675\n",
            "Epoch [75/80], Step [74/7840], Loss: 0.1149\n",
            "Epoch [75/80], Step [74/7840], Loss: 0.0798\n",
            "Epoch [75/80], Step [74/7840], Loss: 0.1108\n",
            "Epoch [75/80], Step [74/7840], Loss: 0.1413\n",
            "Epoch [75/80], Step [74/7840], Loss: 0.1467\n",
            "Epoch [75/80], Step [74/7840], Loss: 0.1088\n",
            "Epoch [75/80], Step [74/7840], Loss: 0.0876\n",
            "Epoch [75/80], Step [74/7840], Loss: 0.0519\n",
            "Epoch [76/80], Step [75/7840], Loss: 0.1426\n",
            "Epoch [76/80], Step [75/7840], Loss: 0.0827\n",
            "Epoch [76/80], Step [75/7840], Loss: 0.0738\n",
            "Epoch [76/80], Step [75/7840], Loss: 0.1899\n",
            "Epoch [76/80], Step [75/7840], Loss: 0.1433\n",
            "Epoch [76/80], Step [75/7840], Loss: 0.1082\n",
            "Epoch [76/80], Step [75/7840], Loss: 0.0802\n",
            "Epoch [76/80], Step [75/7840], Loss: 0.0637\n",
            "Epoch [76/80], Step [75/7840], Loss: 0.1477\n",
            "Epoch [77/80], Step [76/7840], Loss: 0.1036\n",
            "Epoch [77/80], Step [76/7840], Loss: 0.0995\n",
            "Epoch [77/80], Step [76/7840], Loss: 0.0776\n",
            "Epoch [77/80], Step [76/7840], Loss: 0.0908\n",
            "Epoch [77/80], Step [76/7840], Loss: 0.0739\n",
            "Epoch [77/80], Step [76/7840], Loss: 0.0776\n",
            "Epoch [77/80], Step [76/7840], Loss: 0.1301\n",
            "Epoch [77/80], Step [76/7840], Loss: 0.1116\n",
            "Epoch [77/80], Step [76/7840], Loss: 0.1260\n",
            "Epoch [78/80], Step [77/7840], Loss: 0.1465\n",
            "Epoch [78/80], Step [77/7840], Loss: 0.0699\n",
            "Epoch [78/80], Step [77/7840], Loss: 0.1059\n",
            "Epoch [78/80], Step [77/7840], Loss: 0.1134\n",
            "Epoch [78/80], Step [77/7840], Loss: 0.1223\n",
            "Epoch [78/80], Step [77/7840], Loss: 0.0843\n",
            "Epoch [78/80], Step [77/7840], Loss: 0.0790\n",
            "Epoch [78/80], Step [77/7840], Loss: 0.1036\n",
            "Epoch [78/80], Step [77/7840], Loss: 0.1059\n",
            "Epoch [79/80], Step [78/7840], Loss: 0.0669\n",
            "Epoch [79/80], Step [78/7840], Loss: 0.1241\n",
            "Epoch [79/80], Step [78/7840], Loss: 0.0800\n",
            "Epoch [79/80], Step [78/7840], Loss: 0.0857\n",
            "Epoch [79/80], Step [78/7840], Loss: 0.0663\n",
            "Epoch [79/80], Step [78/7840], Loss: 0.0474\n",
            "Epoch [79/80], Step [78/7840], Loss: 0.0763\n",
            "Epoch [79/80], Step [78/7840], Loss: 0.0873\n",
            "Epoch [79/80], Step [78/7840], Loss: 0.0658\n",
            "Epoch [80/80], Step [79/7840], Loss: 0.0735\n",
            "Epoch [80/80], Step [79/7840], Loss: 0.1113\n",
            "Epoch [80/80], Step [79/7840], Loss: 0.1222\n",
            "Epoch [80/80], Step [79/7840], Loss: 0.0838\n",
            "Epoch [80/80], Step [79/7840], Loss: 0.1214\n",
            "Epoch [80/80], Step [79/7840], Loss: 0.1112\n",
            "Epoch [80/80], Step [79/7840], Loss: 0.0890\n",
            "Epoch [80/80], Step [79/7840], Loss: 0.1166\n",
            "Epoch [80/80], Step [79/7840], Loss: 0.0928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "#Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "#Plot the learning curve.\n",
        "plt.plot(loss_list, 'b-o')\n",
        "\n",
        "#Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "ON4jXiRRUDZS",
        "outputId": "793f134c-67fb-42aa-d308-f58744f97030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_encoder_test = data_encoder.iloc[25000:34000, :]"
      ],
      "metadata": {
        "id": "Dksn2leyGWaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_test = create_embeddings_gensim(data_encoder_test, 0, 9000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07jkT0HmiKwY",
        "outputId": "5c3ff8f7-3da9-454e-ab23-d45850d210ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "3500\n",
            "3600\n",
            "3700\n",
            "3800\n",
            "3900\n",
            "4000\n",
            "4100\n",
            "4200\n",
            "4300\n",
            "4400\n",
            "4500\n",
            "4600\n",
            "4700\n",
            "4800\n",
            "4900\n",
            "5000\n",
            "5100\n",
            "5200\n",
            "5300\n",
            "5400\n",
            "5500\n",
            "5600\n",
            "5700\n",
            "5800\n",
            "5900\n",
            "6000\n",
            "6100\n",
            "6200\n",
            "6300\n",
            "6400\n",
            "6500\n",
            "6600\n",
            "6700\n",
            "6800\n",
            "6900\n",
            "7000\n",
            "7100\n",
            "7200\n",
            "7300\n",
            "7400\n",
            "7500\n",
            "7600\n",
            "7700\n",
            "7800\n",
            "7900\n",
            "8000\n",
            "8100\n",
            "8200\n",
            "8300\n",
            "8400\n",
            "8500\n",
            "8600\n",
            "8700\n",
            "8800\n",
            "8900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test = torch.tensor(list(data_encoder_test['toxicity']), dtype=torch.long)\n",
        "test_tensor_dataset = TensorDataset(features_test, labels_test)\n",
        "test_dataloader = torch.utils.data.DataLoader(dataset=test_tensor_dataset, \n",
        "                                           batch_size=BATCH_SIZE, \n",
        "                                           shuffle=False)"
      ],
      "metadata": {
        "id": "ku8qzF4qiKzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    predicted_list = []\n",
        "    for x, labels in test_dataloader:\n",
        "        x = x.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(x)\n",
        "        #max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        predicted_list.extend(predicted.tolist())\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(metrics.accuracy_score(predicted_list, labels_test[:9000].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz_QoAWdiK1w",
        "outputId": "66b1ca9e-7278-4cef-89f6-70e15e48ee4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save model as onnx\n",
        "torch.save(model.state_dict(), 'model_encoder.pth')\n",
        "dummy_input = Variable(torch.randn(1, 25, 300)).cuda()\n",
        "torch.onnx.export(model, dummy_input, \"model_encoder.onnx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ4Q4xcDKGra",
        "outputId": "f81a73ea-3a9f-4932-8914-decdde64210b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/onnx/symbolic_opset9.py:2123: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
            "  \"or define the initial states (h0/c0) as inputs of the model. \")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this code extract encoded vector from n-th layer of nn\n",
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "def get_encoded_sentences(dataloader, model):\n",
        "  model.fc_2.register_forward_hook(get_activation('fc_2'))\n",
        "  vectors = []\n",
        "  #sentence here is vector of embeddings\n",
        "  for i, (sentence_batch, label) in enumerate(dataloader):\n",
        "    for sentence in sentence_batch:\n",
        "      #vectors.append(tensor) \n",
        "      output = model(torch.unsqueeze(sentence, 0))\n",
        "      vectors.append(torch.unsqueeze(activation['fc_2'], 0))\n",
        "  return torch.cat(vectors, dim=0)"
      ],
      "metadata": {
        "id": "GsEEcVHz1D12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_vectors1 = get_encoded_sentences(test_dataloader, model)"
      ],
      "metadata": {
        "id": "y5likIPp1D7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(encoded_vectors1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bU9HqXg1D9r",
        "outputId": "49b307f6-bb7f-49a2-88ac-5a34df7cb0ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9000"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(encoded_vectors1[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5Z6cifJ1EAN",
        "outputId": "a64a95c9-24a2-4f4d-f82b-387f49d54b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mr8qu68X1ECS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_sent.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtY9UIFd9-B4",
        "outputId": "89770d2d-d451-4574-e8b4-d3f0a7f79c73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 25, 96])"
            ]
          },
          "metadata": {},
          "execution_count": 274
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"hello this sentence has more than only 15 words it is definetly has a bit more words tank you\"\n",
        "print(nlp(str(data.iloc[1, 0])).tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBmCbwjBjRxG",
        "outputId": "f7ee9c00-5e9c-4bac-8fa2-4544d043aa1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(15, 96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.iloc[1, 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qCerupo-26ug",
        "outputId": "db662103-0c3d-4cd5-e3b6-27cfba7385f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is the story of Kohinoor (Koh-i-Noor) Diamond?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 271
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.zeros(5,10)\n",
        "tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW8pRrMxhDlD",
        "outputId": "35f5e83c-35dc-41bd-b21a-af3a21a8ba2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor[:, :5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZzWtHgPiZ3D",
        "outputId": "1ada601b-c7af-4181-fda0-15b8fe2616f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset1 = TensorDataset(torch.tensor(data['question1']))"
      ],
      "metadata": {
        "id": "-3cEUc3lXb2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor(list(data['question1']))"
      ],
      "metadata": {
        "id": "74FcBi7tXdAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6IO7Qf59TDqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.iloc[:, 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E9eH26jTEhX",
        "outputId": "d0320e33-75bf-418c-a5cc-98902e0d76fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "0         What is the step by step guide to invest in sh...\n",
              "1         What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
              "2         How can I increase the speed of my internet co...\n",
              "3         Why am I mentally very lonely? How can I solve...\n",
              "4         Which one dissolve in water quikly sugar, salt...\n",
              "                                ...                        \n",
              "128206                            Do women enjoy anal play?\n",
              "128207               Why are people so afraid of socialism?\n",
              "128208    Need career related advice. I am confused as t...\n",
              "128209              Why does everyone hate Hillary Clinton?\n",
              "128210    Why are temperature and average kinetic energy...\n",
              "Name: question1, Length: 128211, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gJXxE6x2XSE4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}